---
title: "Uncertainty in Inflation Forecasting Using Bayesian Methods"
author: Samuel Louissaint
output: html_document
---
# Introduction
The project will be using some macroeconomic knowledge, specifically the Monetarist understanding of economic activity. It's defined by a deceivingly simply formula: ${MV = PQ}$  
According to this definition, M is the money supply, V is the money velocity, P is the price of goods and services, and Q is the quantity of goods and services. Meaning the price of goods can be demonstrated as such: ${P = \frac{MV}{Q}}$  
In terms of economic indicators measured by the Federal Reserve Bank, this formula is best represented by:  
**P - Consumer Price Index for All Urban Consumers: All Items in U.S. City Average (though for this investigation we'll be using the percentage change from previous year figures to represent inflationary movements)**  
**M - Real M2 Money Stock**  
**V - Velocity of M2 Money Stock**  
**Q - Real Gross Domestic Product**  
Luckily, the Fed saved me the trouble of adjusting these figures for CPI myself, and for that I thank them deeply. In terms of a scientific model, there isn't an independent variable we can manipulate freely like in a controlled experiment. However, there is a tool regularly used by the Fed to temper inflation and unemployment, the interest rate. According to macroeconomic theory, increasing interest rates decreases lending/borrowing and thus consumer spending, decreasing inflation at the cost of economic growth. Because it is the only indicator that is so closely related to inflation ***and*** can be directly manipulated --- though often as a response to other economic activity or fiscal policy changes --- it serves as the exposure variable in the following directed acyclic graph (DAG).
```{r, echo=FALSE, results="hide", warning=FALSE}
packages <- c('ggdag', 'dagitty', 'tidyverse', 'tidybayes', 'bsts', 'rstan', 'modelr', 'brms', 'gganimate', 'cowplot', 'ggridges', 'colorspace', 'lubridate', 'rethinking')
suppressPackageStartupMessages(lapply(packages, library, character.only = T))
```

```{r, echo=FALSE, warning=FALSE}
g <- dagitty::dagitty('
dag {
bb="0,0,1,1"
IR [exposure,pos="0.372,0.467"]
M [pos="0.291,0.341"]
P [outcome,pos="0.372,0.341"]
U [latent,pos="0.250,0.538"]
V [pos="0.489,0.538"]
gdp [pos="0.255,0.154"]
IR -> P
M -> P
U -> IR
U -> M
U -> P
U -> V
U -> gdp
V -> IR
V -> P
gdp -> P
}
')
ggdag::ggdag_status(g, text = FALSE, use_labels = "name") +
  theme_dag()
```
  
It is crucial to remember that this model, like all models, is a simplified portrayal of the interactions it is meant to describe. Due to the compounding impacts of exogenous shocks and indicators outside the scope of the model, we cannot model the direct effect of the interest rate on inflation. However, we are able to model the total effect of the interest rate on inflation if we control for money velocity.  
  
The next step will be to create our priors and build a generative model. The loop I crafted to do so has its limitations, though I wasn't able to find any functions or loops to use as reference. The goal is to create time series that behaves similarly to the indicators they're meant to mimic.
# Generative Model
## Forming data from priors
```{r}
par(mfrow=c(2,2))
set.seed(9)
N <- 600 # Number of observations
time <- seq(1, N, 1) # Time vector
P <- c(4.5) # "true" mu
for(i in seq(599)){
  p <- rnorm(1, mean = P[i], rnorm(1, 0.3,0.1)) # Makes sure next point stems from previous
  P <- c(P, p)}
P <- P/100
plot(time, P, 'l')

set.seed(18)
IR <- c(1)
for(i in seq(599)){
  ir <- round(rnorm(1, mean = IR[i], 0.04),1)
  IR <- c(IR, ir)}
IR <- IR / 100
plot(time, IR, 'l')

set.seed(4)
V <- c(2)
for(i in seq(599)){
  v <- round(rnorm(1, mean = V[i], 0.04), 2)
  V <- c(V, v)}
plot(time, V, 'l')
```
  
Here we have simulated data for the indicators going into the model. We use those to model the relationship using the priors we set as well as priors included in the Bayesian structure time series (bsts) package.  
  
## Prior Predictive Check - Visualize Priors
```{r, results='hide'}
gm <- cbind.data.frame(time, P, IR, V)
gm_test <- gm[501:600,]
gm <- gm[1:500,]
gmod_components <- list()
gmod_components <- AddStudentLocalLinearTrend(gmod_components, y = gm$P)
gmod_components <- AddSeasonal(gmod_components, y = gm$P, nseasons = 50, season.duration = 4)
gmod <- bsts(P~IR+V, gmod_components, niter = 5000, data = gm)
```

```{r}
gpred <- predict(gmod, horizon = 10, gm_test)
plot(gpred, ylim= c(-0.1, 0.1))
```

```{r, warning=FALSE}
forecast_time = 100
fits = gm %>%
  add_draws(colSums(aperm(gmod$state.contributions, c(2, 1, 3))))

predictions = data.frame(time = max(gm$time) + 1:forecast_time) %>%
  add_draws(predict(gmod, newdata = gm_test, horizon = forecast_time)$distribution, value = ".prediction")

predictions_with_last_obs = gm %>% 
  slice(n()) %>% 
  mutate(.draw = list(1:max(predictions$.draw))) %>% 
  unnest(cols = c(.draw)) %>% 
  mutate(.prediction = P) %>% 
  bind_rows(predictions)
gm %>%
  ggplot(aes(x = time, y = P)) +
  geom_line(aes(y = .value, group = .draw), alpha = 1/20, data = fits %>% sample_draws(100)) +
  geom_line(aes(y = .prediction, group = .draw), alpha = 1/20, data = predictions %>% sample_draws(100)) +
  geom_point() +
  ylim(c(-0.06,0.06))
```
  
We see from the prior predictives that the model and priors are behaving in a way we'd expect. The values themselves aren't what you'd expect from inflation though this a result of struggling to find rnorm values that looked like the inflation curve. We may attribute the fuzziness in predictions through even the observed time values to the fact that the generated priors aren't actually related. Additionally, seeing such a wide range of prior predicted values is expected and satisfactory for something as unpredictable as the economy.
  
  
With a working set of priors, we can now move onto modeling real data...or that is what I'd like to say. However, the real economic indicators that the priors represent do not exactly in the way described. Specifically, in terms of timing. The interest rate is a reactive indicator, meaning it moves *because* inflation moved. Thus there was the need to create the DAG below, which introduces the concept of time to the model.

```{r, echo=FALSE, warning=FALSE}
g <- dagitty::dagitty('
dag {
bb="0,0,1,1"
"P-1" [pos="0.080,0.082"]
"IR-1" [exposure,pos="0.078,0.219"]
P [outcome,pos="0.487,0.082"]
IR [pos="0.487,0.223"]
M [pos="0.417,0.416"]
U [latent,pos="0.254,0.539"]
V [pos="0.282,0.340"]
gdp [pos="0.248,0.272"]
"P-1" -> "IR-1"
"P-1" -> P
"IR-1" -> P
"IR-1" -> IR
"IR-1" -> gdp
P -> IR
M -> I
U -> I
U -> M
U -> V
U -> gdp
V -> "IR"
V -> P
gdp -> P
}')
ggdag::ggdag_status(g, text = FALSE, use_labels = "name") +
  theme_dag()
```
  
# Modeling with Real Data
```{r, warning=FALSE}
# Load data
gdp_file = "https://github.com/SamL153/bayesian_stats_project/raw/master/gdp.csv"
inflation_file = "https://github.com/SamL153/bayesian_stats_project/raw/master/inflation_rate.csv"
interest_file = "https://github.com/SamL153/bayesian_stats_project/raw/master/interest_rate.csv"
msupply_file = "https://github.com/SamL153/bayesian_stats_project/raw/master/money_supply.csv"
mvelocity_file = "https://github.com/SamL153/bayesian_stats_project/raw/master/money_velocity.csv"
Q = read_csv(gdp_file, show_col_types = F) %>%
  filter(DATE >= as.POSIXlt('1980-01-01'))
P = read_csv(inflation_file, show_col_types = F) %>%
  filter(DATE >= as.POSIXlt('1980-01-01')) %>%
  mutate(CPIAUCSL_PC1 = as.double(CPIAUCSL_PC1)/100)
I = read_csv(interest_file, show_col_types = F) %>%
  filter(DATE >= as.POSIXlt('1980-01-01'), !is.na(DFF)) %>%
  mutate(DFF = as.double(DFF)/100)
M = read_csv(msupply_file, show_col_types = F) %>%
  filter(DATE >= as.POSIXlt('1980-01-01'))
V = read_csv(mvelocity_file, show_col_types = F) %>%
  filter(DATE >= as.POSIXlt('1980-01-01'))
monthly_train <- P %>% inner_join(I, by = 'DATE') %>% inner_join(M, by = 'DATE') %>% left_join(Q, by = 'DATE') %>% left_join(V, by = 'DATE') %>%
  rename(P = CPIAUCSL_PC1, I = DFF, M = M2REAL, Q = GDPC1, V = M2V) %>%
  filter(DATE < as.POSIXlt('2021-07-01')) %>%
  mutate(m = month(DATE),
         V = na.spline(V),
         Q = na.spline(Q))
monthly_test <- P %>% inner_join(I, by = 'DATE') %>% inner_join(M, by = 'DATE') %>% left_join(Q, by = 'DATE') %>% left_join(V, by = 'DATE') %>%
  rename(P = CPIAUCSL_PC1, I = DFF, M = M2REAL, Q = GDPC1, V = M2V) %>%
  filter(DATE >= as.POSIXlt('2021-07-01')) %>%
  mutate(m = month(DATE),
         V = na.spline(V),
         Q = na.spline(Q))
I_m1_train <- rbind(I[1,], I) %>%
  filter(DATE < as.POSIXlt('2021-06-01')) %>%
  rename(I_m1 = DFF)
I_m1_test <- I %>%
  filter(DATE >= as.POSIXlt('2021-06-01'), DATE < as.POSIXlt('2022-03-01')) %>%
  rename(I_m1 = DFF)
P_m1_train <- read_csv(inflation_file, show_col_types = F) %>%
  filter(DATE >= as.POSIXlt('1979-12-01'), DATE < as.POSIXlt('2021-06-01')) %>%
  mutate(CPIAUCSL_PC1 = as.double(CPIAUCSL_PC1)/100) %>%
  rename(P_m1 = CPIAUCSL_PC1)
P_m1_test <- (read_csv(inflation_file, show_col_types = F) %>%
  filter(DATE >= as.POSIXlt('2021-06-01'), DATE < as.POSIXlt('2022-03-01')) %>%
  mutate(CPIAUCSL_PC1 = as.double(CPIAUCSL_PC1)/100) %>%
  rename(P_m1 = CPIAUCSL_PC1))
monthly_train <- cbind(monthly_train, P_m1_train[,2], I_m1_train[,2]) %>%
  mutate(time = 1:n())
monthly_test <- cbind(monthly_test, P_m1_test[,2], I_m1_test[,2]) %>%
  mutate(time = 1:n())
```
Due to the increased complexity of the scientific model, a few variables need to be created or changed. Including the creation of "T-1" columns, for inflation and interest rates. As well as imputing with "na.spline" for data that only releases figures quarterly.  
  
## Fit Model
The model we will be fitting and analyzing is the total effect of interest rate on inflation, which consists of only the time lagged variables. Additionally, we removed the seasonal trend prior, as it didn't impact the model in a practical or theoretical way.
```{r, results='hide'}
model_components <- list()
model_components <- AddStudentLocalLinearTrend(model_components, y =  monthly_train$P)
model <- bsts(P~I_m1+P_m1, model_components, niter = 5000, data = monthly_train)
```


```{r, echo=FALSE}
forecast_months = 9 # number of months forward to forecast
set.seed(123456)
y_max = .1
y_axis = list(
  coord_cartesian(ylim = c(-0.02, y_max), expand = FALSE),
  scale_y_continuous(labels = scales::percent),
  theme(axis.text.y = element_text(vjust = 0.05))
)
title = labs(x = NULL, y = NULL, subtitle = "US inflation over time")

fits = monthly_train %>%
  add_draws(colSums(aperm(model$state.contributions, c(2, 1, 3))))
predictions = monthly_train %$%
  tibble(
    DATE = max(DATE) + months(1:forecast_months),
    m = month(DATE),
    time = max(time) + 1:forecast_months
  ) %>%
  add_draws(predict(model, newdata = monthly_test, horizon = forecast_months)$distribution, value = ".prediction")
predictions_with_last_obs = monthly_train %>% 
  slice(n()) %>% 
  mutate(.draw = list(1:max(predictions$.draw))) %>% 
  unnest(cols = c(.draw)) %>% 
  mutate(.prediction = P) %>% 
  bind_rows(predictions)
```
A first glance at the model's predictive ability below shows its effectiveness at plotting both what has been and what may be, though we'll take a closer look at the more recent figures going forward.
```{r}
monthly_train %>%
  ggplot(aes(x = DATE, y = P)) +
  geom_line(aes(y = .value, group = .draw), alpha = 1/20, data = fits %>% sample_draws(100)) +
  geom_line(aes(y = .prediction, group = .draw), alpha = 1/20, data = predictions %>% sample_draws(100)) +
  geom_point()+
  coord_cartesian(ylim = c(-0.02, 0.15))
```

# Model Diagnostics
Unfortunately, the bsts package is relatively small and doesn't have very many diagnostics functions like traceplots and number of effective sample size. However, we are able to judge convergence of the model by estimating the one step ahead prediction error variance by squaring the error provided my the bsts model and plotting. Additionally, we see that we have fairly independent chains seen through the autocorrelation plot.
```{r, fig.height=6}
par(mfrow=c(2,1))
a <- rowMeans(model$one.step.prediction.errors ^ 2)
plot(a, type = 'l')
r <- residuals(model)
AcfDist(r)
```

# Posterior Sampling and Updated Priors
Moving on, the plot below is the fit and forecast starting from 2014. The points are the actual values from the training set, while the blue line is the posterior predictives of the data, having been trained on it. The more interesting portion is the scattering red lines, which are updated prior predictives based on the model fit thus far.
```{r, echo = FALSE}
since_year = 2014
set.seed(123456)
fit_color = "#3573b9"
fit_color_fill = hex(mixcolor(.6, sRGB(1,1,1), hex2RGB(fit_color)))
prediction_color = "#e41a1c"
prediction_color_fill = hex(mixcolor(.6, sRGB(1,1,1), hex2RGB(prediction_color)))

x_axis = list(
  scale_x_date(date_breaks = "1 years", labels = year),
  theme(axis.text.x = element_text(hjust = 0.1))
)

monthly_train %>%
  filter(year(DATE) >= since_year) %>%
  ggplot(aes(x = DATE, y = P)) +
  geom_line(aes(y = .value, group = .draw), alpha = 1/30, color = fit_color, size = .75, # For each year since 2014, take 100 samples and use those to create lines which are blurred to show what i believe is the posterior predictives for each year.
    data = fits %>% filter(year(DATE) >= since_year) %>% sample_draws(100)) +
  geom_line(aes(y = .prediction, group = .draw), alpha = 1/20, color = prediction_color, size = .75, # Does prior predictives based on fit. Clearly they scatter more
    data = predictions %>% sample_draws(100)) + # point estimates from initial data
  geom_point(size = 0.75) +
  y_axis +
  x_axis +
  title
```

This plot was created for a clearer image, as well as a comparison to actual inflation figures during the same period, roughly from July 2021 to February 2022. Due to the use of the Student T distribution, the uncertainty *does* cover the trajectory of the observed inflation. However, it also suspected a somewhat rapid recovery that doesn't look very plausible either.
```{r}
monthly_train %>%
  filter(year(DATE) >= since_year) %>%
  ggplot(aes(x = DATE, y = P)) +
  stat_lineribbon(aes(y = .value), fill = adjustcolor(fit_color, alpha.f = .25), color = fit_color, .width = .95,
    data = fits %>% filter(year(DATE) >= since_year)) +
  stat_lineribbon(aes(y = .prediction), fill = adjustcolor(prediction_color, alpha.f = .25), color = prediction_color, .width = .95,
    data = predictions) +
  geom_line(aes(y= P), data = monthly_test)+
  geom_point(size = 0.75) +
  y_axis +
  x_axis +
  title
```
The plot demonstrates the model fit as usual in blue, though in red shows 50 equally likely points at the specified time. Notice how quickly the wide the dispersion is in the November and February forecasts. Obviously this is typically a sign of low confidence in future values, though the February distribution *does* include the actual inflation rate which was around 0.079.
```{r, echo = FALSE}
fit_plot = monthly_train %>%
  filter(year(DATE) >= since_year) %>%
  ggplot(aes(x = DATE, y = P)) +
  geom_line(color = "gray75") +
  geom_point(size = 0.75) +
  y_axis +
  x_axis +
  expand_limits(x = ymd("2019-06-01")) +
  title

facet_x_labels = list(
  geom_vline(xintercept = 0, color = "gray75"),
  theme(
    strip.text.x = element_text(hjust = 0, size = 7.5, margin = margin(3, 1, 5, 1)),
    panel.spacing.x = unit(3, "points"),
    axis.text.y = element_text(vjust = 0.05),
    strip.background = element_blank(),
    plot.margin = margin(0, 3, 0, 0)
  )
)

predict_plot = predictions %>%
  filter(DATE %in% c(ymd("2021-07-01"), ymd("2021-11-01"), ymd("2022-02-01"))) %>%
  group_by(DATE) %>%
  do(tibble(.prediction = quantile(.$.prediction, ppoints(50)))) %>%
  ggplot(aes(x = .prediction)) +
  geom_hline(yintercept = 0, color = "gray85", linetype = "dashed") +
  geom_dotplot(fill = prediction_color_fill, color = NA, binwidth = .001, dotsize = 1.1) +
  ylab(NULL) +
  xlab(NULL) +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = NULL) +
  coord_flip(xlim = c(0, y_max), expand = FALSE) +
  facet_grid(. ~ DATE, labeller = labeller(DATE = function(x) strftime(x, "%b\n%Y")), switch = "x", scales = "free_x") +
  facet_x_labels

plot_grid(align = "h", axis = "tb", ncol = 2, rel_widths = c(4, 1),
    fit_plot,
    predict_plot
  )
```

# Stratification and Model Comparison
The model we've been look at thus far is the total effect of interest rate on inflation rate. Now we'll briefly take a look at the direct effect, which now also stratifies for money velocity, GDP. In addition to this, we'll be developing a model using all of the variables, to see whether this has a positive or negative impact on prediction.
```{r, results='hide'}
direct_model = bsts(P~I_m1+V+P_m1+Q, model_components, niter = 5000, data = monthly_train)
allvar_model <- bsts(P~I_m1+V+P_m1+M+Q+I, model_components, niter = 5000, data = monthly_train)
```


```{r, fig.height=6}
CompareBstsModels(list('Total Effect Model' = model,
                       'Direct Effect Model' = direct_model,
                       'All Var Model' = allvar_model), burn = 200, main = 'Model Prediction Error Comparison')
```

This unexpected increase in error may be due largely to what's described in the plots below. You can clearly see that both newer models have a high inclusion probability for money velocity and likely uses it to over-explain variation.
```{r, echo=FALSE}
par(mfrow=c(2,2))
plot(model, 'coefficients', main = 'Total Effect Model')
plot(direct_model, 'coefficients', main = 'Direct Effect Model')
plot(allvar_model, 'coefficients', main = 'All Variables')
```

Lastly, point estimates aren't quite the best use of Bayesian methods, they're included here. These values describe:  
The posterior probability that the variable is positive.  
The conditional expectation of the coefficient, given inclusion.  
The conditional standard deviation of the coefficient, given inclusion  
The posterior probability the variable is included.  
You'll notice that the expected coefficient is positive, which is the opposite of what's expected given prior knowledge of macroeconomics. My best attempt at explaining this would be that the lag used in inflation is simply too short so the model misinterpreted the relationship.
```{r}
summary(model)[6]
```
